{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A decision boundary between the 2 classes is   created. \n",
    "1. SVM learns how important each training data point is in representing the decision boundary between the 2 classes.\n",
    "1. Typicaly only a subset of the training points matter for defining the decision boundary.\n",
    "1. The one that lie on the border between the classes.\n",
    "1. These are caled support vectors.\n",
    "1. To make a prediciton for a new point, the distance to each of the support vectors is measured.\n",
    "1. A classification decision is based on the distances to the support vector, and the importance of the support vectors that was learned during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning SVM parameters:\n",
    "1. The gamma parameter- controls of the width of the Gaussian kernel.\n",
    "1. It determines the scale of what it means for points to be close together.\n",
    "1. The C parameter- regularization parameter- It limits the importance of each point in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gamma parameter:\n",
    "1. Small gamma means the decision boundary will vary slowly, which yields a model of low complexity.Reflected as a very smooth decision boundary.\n",
    "1. A high gamma means the boundaries are focussed more on single points, and yields a complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C parameter:\n",
    "1. Small C means a very restricted model, where each data point will have very limited influence.\n",
    "1. This means the misclassified points barely have any influence on the line.\n",
    "1. A high C allows the misclassified points to have a stronger influence on the madel and makes the decision boundary bend to correctly classify them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "1. SVMs perform well on a variety of datasets, allowing for complex decision boundaries, even if the data has only a few features.\n",
    "1. Works well on data that have high either few or many features.\n",
    "1. If all of the features represent measurements in similar units and are on similar scale, SVM are worth the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weaknesses:\n",
    "1. SVMs dont scale well with the number of samples.\n",
    "1. Working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\n",
    "1. Requires careful preprocessing of the data and tuning of parameters. Tree based models such as random forests or gradient boosting(which require little or no preprocessing) are better alternatives. \n",
    "1. SVMs are hard to inspect and might be tricy to explain the model to a nonexpert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_names', 'data', 'target', 'DESCR', 'feature_names']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malignant', 212), ('benign', 357)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "zip(cancer.target_names,np.bincount(cancer.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension'], \n",
       "      dtype='|S23')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name model_selection",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-9779e327a0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#import train_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name model_selection"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "#import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest=train_test_split(cancer.data,cancer.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm=svc(C=100)\n",
    "svm.fit(Xtrain.ytrain)\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(Xtest,ytest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing using 0-1 scaling\n",
    "scaler= MinMaxScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtest_scales = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm.fit(Xtrain_scaled,ytrain)\n",
    "print(\"Scaled test set accuracy: {:.2f}\".format(svm.score(Xtest_scaled,ytest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing using StandardScaler(zero mean and unit variance scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain_scaled=scaler.transform(Xtrain)\n",
    "Xtest_scaled=scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm.fit(Xtrain,ytain)\n",
    "print(\"Standard Scaled test set accuracy: {:.2f}\".format(svm.score(Xtest_scaled,ytest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
