{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as NB\n",
    "import numpy as np,pandas as pd\n",
    "\n",
    "#import data into a nu,py array\n",
    "X=np.array([[-1,-1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])\n",
    "y=np.array([1,1,1,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorize=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1]\n",
      " [-2 -1]\n",
      " [-3 -2]\n",
      " [ 1  1]\n",
      " [ 2  1]\n",
      " [ 3  2]]\n",
      "[1 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# view data\n",
    "print X\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize 'clf' as the Gaussian Naive Bayes classifier and fit\n",
    "\n",
    "clf = NB.GaussianNB()\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "# predict a few instances\n",
    "print(clf.predict([[-0.8,-1],[-1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#???????\n",
    "clf_pf = NB.GaussianNB()\n",
    "clf_pf.partial_fit(X,y,np.unique(y))\n",
    "print(clf_pf.predict([[-0.8,-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "df=pd.read_csv(url,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53, 54, 55, 56, 57],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(57,axis=1)\n",
    "y=df[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts() #spam=1 and ham=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.605955\n",
       "1    0.394045\n",
       "Name: 57, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.fit(Xtrain,ytrain)\n",
    "    ypred = model.predict(Xtest)\n",
    "    a=accuracy_score(ytest, ypred)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.787141615986099, 0.81754995655951346, 0.89139878366637704]\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB()\n",
    "models =[mnb,gnb,bnb]\n",
    "score=[]\n",
    "for model in models:\n",
    "    score.append(evaluate(model))\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787141615986\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(Xtrain,ytrain)\n",
    "ypred = mnb.predict(Xtest)\n",
    "mnb_score = accuracy_score(ytest,ypred)\n",
    "print mnb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81754995656\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain,ytrain)\n",
    "ypred = gnb.predict(Xtest)\n",
    "gnb_score = accuracy_score(ytest,ypred)\n",
    "print gnb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891398783666\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(Xtrain,ytrain)\n",
    "ypred = bnb.predict(Xtest)\n",
    "bnb_score = accuracy_score(ytest,ypred)\n",
    "print bnb_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - Representing text as data\n",
    "# - Reading SMS data\n",
    "# - Vectorizing SMS data\n",
    "# - Examining the tokens and their counts\n",
    "# - Bonus: Calculating the \"spamminess\" of each token\n",
    "# \n",
    "# **Naive Bayes classification**\n",
    "# \n",
    "# - Building a Naive Bayes model\n",
    "# - Comparing Naive Bayes with logistic regression\n",
    "\n",
    "# ## Part 1: Representing text as data\n",
    "# \n",
    "# From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "# \n",
    "# > Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "# \n",
    "# We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# start with a simple example\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']\n",
    "\n",
    "\n",
    "# learn the 'vocabulary' of the training data\n",
    "vect = CountVectorizer()\n",
    "vect.fit(simple_train)\n",
    "vect.get_feature_names()\n",
    "\n",
    "\n",
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm\n",
    "\n",
    "\n",
    "# print the sparse matrix\n",
    "print simple_train_dtm\n",
    "\n",
    "\n",
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()\n",
    "\n",
    "\n",
    "# examine the vocabulary and document-term matrix together\n",
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "\n",
    "# From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "# \n",
    "# > In this scheme, features and samples are defined as follows:\n",
    "# \n",
    "# > - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "# > - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "# \n",
    "# > A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "# \n",
    "# > We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test = [\"please don't call me\"]\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()\n",
    "\n",
    "\n",
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "\n",
    "# **Summary:**\n",
    "# \n",
    "# - `vect.fit(train)` learns the vocabulary of the training data\n",
    "# - `vect.transform(train)` uses the fitted vocabulary to build a document-term matrix from the training data\n",
    "# - `vect.transform(test)` uses the fitted vocabulary to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)\n",
    "\n",
    "# ## Part 2: Reading SMS data\n",
    "\n",
    "# read tab-separated file\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv'\n",
    "col_names = ['label', 'message']\n",
    "sms = pd.read_table(url, sep='\\t', header=None, names=col_names)\n",
    "print sms.shape\n",
    "\n",
    "\n",
    "sms.head(20)\n",
    "\n",
    "\n",
    "sms.label.value_counts()\n",
    "\n",
    "\n",
    "# convert label to a numeric variable\n",
    "sms['label'] = sms.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "\n",
    "# define X and y\n",
    "X = sms.message\n",
    "y = sms.label\n",
    "\n",
    "\n",
    "# split into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "\n",
    "\n",
    "# ## Part 3: Vectorizing SMS data\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "\n",
    "# learn training data vocabulary, then create document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm\n",
    "\n",
    "\n",
    "# alternative: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm\n",
    "\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "\n",
    "# ## Part 4: Examining the tokens and their counts\n",
    "\n",
    "# store token names\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "\n",
    "\n",
    "# first 50 tokens\n",
    "print X_train_tokens[:50]\n",
    "\n",
    "\n",
    "# last 50 tokens\n",
    "print X_train_tokens[-50:]\n",
    "\n",
    "\n",
    "# view X_train_dtm as a dense matrix\n",
    "X_train_dtm.toarray()\n",
    "\n",
    "\n",
    "# count how many times EACH token appears across ALL messages in X_train_dtm\n",
    "import numpy as np\n",
    "X_train_counts = np.sum(X_train_dtm.toarray(), axis=0)\n",
    "X_train_counts\n",
    "\n",
    "\n",
    "X_train_counts.shape\n",
    "\n",
    "\n",
    "# create a DataFrame of tokens with their counts\n",
    "pd.DataFrame({'token':X_train_tokens, 'count':X_train_counts}).sort('count')\n",
    "\n",
    "\n",
    "# ## Bonus: Calculating the \"spamminess\" of each token\n",
    "\n",
    "# create separate DataFrames for ham and spam\n",
    "sms_ham = sms[sms.label==0]\n",
    "sms_spam = sms[sms.label==1]\n",
    "\n",
    "\n",
    "# learn the vocabulary of ALL messages and save it\n",
    "vect.fit(sms.message)\n",
    "all_tokens = vect.get_feature_names()\n",
    "\n",
    "\n",
    "# create document-term matrices for ham and spam\n",
    "ham_dtm = vect.transform(sms_ham.message)\n",
    "spam_dtm = vect.transform(sms_spam.message)\n",
    "\n",
    "\n",
    "# count how many times EACH token appears across ALL ham messages\n",
    "ham_counts = np.sum(ham_dtm.toarray(), axis=0)\n",
    "\n",
    "\n",
    "# count how many times EACH token appears across ALL spam messages\n",
    "spam_counts = np.sum(spam_dtm.toarray(), axis=0)\n",
    "\n",
    "\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "token_counts = pd.DataFrame({'token':all_tokens, 'ham':ham_counts, 'spam':spam_counts})\n",
    "\n",
    "\n",
    "# add one to ham and spam counts to avoid dividing by zero (in the step that follows)\n",
    "token_counts['ham'] = token_counts.ham + 1\n",
    "token_counts['spam'] = token_counts.spam + 1\n",
    "\n",
    "\n",
    "# calculate ratio of spam-to-ham for each token\n",
    "token_counts['spam_ratio'] = token_counts.spam / token_counts.ham\n",
    "token_counts.sort('spam_ratio')\n",
    "\n",
    "\n",
    "# ## Part 5: Building a Naive Bayes model\n",
    "# \n",
    "# We will use [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "# \n",
    "# > The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "# train a Naive Bayes model using X_train_dtm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "\n",
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "print metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# predict (poorly calibrated) probabilities\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob\n",
    "\n",
    "\n",
    "# calculate AUC\n",
    "print metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "\n",
    "# print message text for the false positives\n",
    "X_test[y_test < y_pred_class]\n",
    "\n",
    "\n",
    "# print message text for the false negatives\n",
    "X_test[y_test > y_pred_class]\n",
    "\n",
    "\n",
    "# what do you notice about the false negatives?\n",
    "X_test[3132]\n",
    "\n",
    "\n",
    "# ## Part 6: Comparing Naive Bayes with logistic regression\n",
    "\n",
    "# import/instantiate/fit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "\n",
    "\n",
    "# class predictions and predicted probabilities\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "\n",
    "# calculate accuracy and AUC\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "print metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer()\n",
    "Xtrain_v=vect.fit_transform(Xtrain)\n",
    "Xtest_v=vect.transform(Xtest)\n",
    "\n",
    "#lower case is false\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "\n",
    "# ngram 1,2\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "\n",
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]\n",
    "\n",
    "\n",
    "# **Predicting the star rating:**\n",
    "\n",
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())\n",
    "\n",
    "\n",
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print 'Features: ', X_train_dtm.shape[1]\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print 'Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# ## Part 3: Stopword Removal\n",
    "\n",
    "# - **What:** Remove common words that will likely appear in any text\n",
    "# - **Why:** They don't tell you much about your text\n",
    "\n",
    "# show vectorizer options\n",
    "vect\n",
    "\n",
    "\n",
    "# - **stop_words:** string {'english'}, list, or None (default)\n",
    "# - If 'english', a built-in stop word list for English is used.\n",
    "# - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "# - If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# set of stop words\n",
    "print vect.get_stop_words()\n",
    "\n",
    "\n",
    "# ## Part 4: Other CountVectorizer Options\n",
    "\n",
    "# - **max_features:** int or None, default=None\n",
    "# - If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# all 100 features\n",
    "print vect.get_feature_names()\n",
    "\n",
    "\n",
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# - **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "# - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "\n",
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# ## Part 5: Introduction to TextBlob\n",
    "\n",
    "# TextBlob: \"Simplified Text Processing\"\n",
    "\n",
    "# print the first review\n",
    "print yelp_best_worst.text[0]\n",
    "\n",
    "\n",
    "# save it as a TextBlob object\n",
    "review = TextBlob(yelp_best_worst.text[0])\n",
    "\n",
    "\n",
    "# list the words\n",
    "review.words\n",
    "\n",
    "\n",
    "# list the sentences\n",
    "review.sentences\n",
    "\n",
    "\n",
    "# some string methods are available\n",
    "review.lower()\n",
    "\n",
    "\n",
    "# ## Part 6: Stemming and Lemmatization\n",
    "\n",
    "# **Stemming:**\n",
    "# \n",
    "# - **What:** Reduce a word to its base/stem/root form\n",
    "# - **Why:** Often makes sense to treat related words the same way\n",
    "# - **Notes:**\n",
    "#     - Uses a \"simple\" and fast rule-based approach\n",
    "#     - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "#     - Some search engines treat words with the same stem as synonyms\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print [stemmer.stem(word) for word in review.words]\n",
    "\n",
    "\n",
    "# **Lemmatization**\n",
    "# \n",
    "# - **What:** Derive the canonical form ('lemma') of a word\n",
    "# - **Why:** Can be better than stemming\n",
    "# - **Notes:** Uses a dictionary-based approach (slower than stemming)\n",
    "\n",
    "# assume every word is a noun\n",
    "print [word.lemmatize() for word in review.words]\n",
    "\n",
    "\n",
    "# assume every word is a verb\n",
    "print [word.lemmatize(pos='v') for word in review.words]\n",
    "\n",
    "\n",
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]\n",
    "\n",
    "\n",
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)\n",
    "\n",
    "\n",
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]\n",
    "\n",
    "\n",
    "# ## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "# - **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "# - **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "# - **Notes:** Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']\n",
    "\n",
    "\n",
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf\n",
    "\n",
    "\n",
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())\n",
    "\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df\n",
    "\n",
    "\n",
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "\n",
    "# **More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)\n",
    "\n",
    "# ## Part 8: Using TF-IDF to Summarize a Yelp Review\n",
    "# \n",
    "# Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!\n",
    "\n",
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape\n",
    "\n",
    "\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP SCORING WORDS:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print word\n",
    "    \n",
    "    # print 5 random words\n",
    "    print '\\n' + 'RANDOM WORDS:'\n",
    "    random_words = np.random.choice(word_scores.keys(), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print word\n",
    "    \n",
    "    # print the review\n",
    "    print '\\n' + review_text\n",
    "\n",
    "\n",
    "summarize()\n",
    "\n",
    "\n",
    "# ## Part 9: Sentiment Analysis\n",
    "\n",
    "print review\n",
    "\n",
    "\n",
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity\n",
    "\n",
    "\n",
    "# understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)\n",
    "\n",
    "\n",
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity\n",
    "\n",
    "\n",
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)\n",
    "\n",
    "\n",
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')\n",
    "\n",
    "\n",
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()\n",
    "\n",
    "\n",
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()\n",
    "\n",
    "\n",
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)\n",
    "\n",
    "\n",
    "# negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)\n",
    "\n",
    "\n",
    "# positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)\n",
    "\n",
    "\n",
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')\n",
    "\n",
    "\n",
    "# ## Bonus: Adding Features to a Document-Term Matrix\n",
    "\n",
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "\n",
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print X_train_dtm.shape\n",
    "print X_test_dtm.shape\n",
    "\n",
    "\n",
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape\n",
    "\n",
    "\n",
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape\n",
    "\n",
    "\n",
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape\n",
    "\n",
    "\n",
    "# repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape\n",
    "\n",
    "\n",
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "\n",
    "# ## Bonus: Fun TextBlob Features\n",
    "\n",
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()\n",
    "\n",
    "\n",
    "# spellcheck\n",
    "Word('parot').spellcheck()\n",
    "\n",
    "\n",
    "# definitions\n",
    "Word('bank').define('v')\n",
    "\n",
    "\n",
    "# language identification\n",
    "TextBlob('Hola amigos').detect_language()\n",
    "\n",
    "\n",
    "# ## Conclusion\n",
    "# \n",
    "# - NLP is a gigantic field\n",
    "# - Understanding the basics broadens the types of data you can work with\n",
    "# - Simple techniques go a long way\n",
    "# - Use scikit-learn for NLP whenever possible"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
